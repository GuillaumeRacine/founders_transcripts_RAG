# Model-specific configurations for different use cases
default_configs:
  research_analysis:
    temperature: 0.1
    max_tokens: 4000
    description: "Conservative settings for analytical research tasks"
  
  creative_content:
    temperature: 0.3
    max_tokens: 3000
    description: "Slightly more creative for content generation"
  
  exploration:
    temperature: 0.4
    max_tokens: 2000
    description: "More exploratory for brainstorming and discovery"

model_specific_optimizations:
  openai:
    gpt-4-turbo:
      best_for: ["complex_analysis", "research_synthesis", "psychological_insights"]
      temperature_range: [0.0, 0.3]
      optimal_temperature: 0.1
      max_context_tokens: 128000
      max_output_tokens: 4096
      
    gpt-4:
      best_for: ["detailed_analysis", "comparative_studies"]
      temperature_range: [0.0, 0.4]
      optimal_temperature: 0.15
      max_context_tokens: 8192
      max_output_tokens: 4096
    
    gpt-3.5-turbo:
      best_for: ["quick_queries", "summarization"]
      temperature_range: [0.1, 0.5]
      optimal_temperature: 0.2
      max_context_tokens: 16385
      max_output_tokens: 4096

  anthropic:
    claude-3-5-sonnet-20241022:
      best_for: ["nuanced_analysis", "ethical_considerations", "long_form_content"]
      temperature_range: [0.0, 0.3]
      optimal_temperature: 0.1
      max_context_tokens: 200000
      max_output_tokens: 4096
      
    claude-3-opus-20240229:
      best_for: ["complex_reasoning", "creative_insights", "comprehensive_analysis"]
      temperature_range: [0.0, 0.4]
      optimal_temperature: 0.15
      max_context_tokens: 200000
      max_output_tokens: 4096

  ollama:
    llama3.1:8b:
      best_for: ["local_processing", "privacy_focused", "quick_insights"]
      temperature_range: [0.1, 0.6]
      optimal_temperature: 0.2
      context_length: 8192
      
    llama3.1:70b:
      best_for: ["complex_local_analysis", "detailed_reasoning"]
      temperature_range: [0.0, 0.4]
      optimal_temperature: 0.1
      context_length: 8192
      
    codellama:
      best_for: ["technical_analysis", "structured_data"]
      temperature_range: [0.0, 0.3]
      optimal_temperature: 0.05
      context_length: 16384

research_task_mappings:
  psychology_analysis:
    recommended_models: ["claude-3-5-sonnet-20241022", "gpt-4-turbo"]
    temperature: 0.1
    max_tokens: 4000
    
  biography_insights:
    recommended_models: ["claude-3-opus-20240229", "gpt-4-turbo"]
    temperature: 0.15
    max_tokens: 3500
    
  trend_identification:
    recommended_models: ["gpt-4-turbo", "claude-3-5-sonnet-20241022"]
    temperature: 0.2
    max_tokens: 3000
    
  story_research:
    recommended_models: ["claude-3-5-sonnet-20241022", "gpt-4"]
    temperature: 0.25
    max_tokens: 3000
    
  comparison_analysis:
    recommended_models: ["gpt-4-turbo", "claude-3-opus-20240229"]
    temperature: 0.1
    max_tokens: 4000

performance_profiles:
  speed_optimized:
    preferred_models: ["gpt-3.5-turbo", "llama3.1:8b"]
    max_tokens: 2000
    temperature: 0.2
    
  quality_optimized:
    preferred_models: ["claude-3-opus-20240229", "gpt-4-turbo"]
    max_tokens: 4000
    temperature: 0.1
    
  cost_optimized:
    preferred_models: ["llama3.1:8b", "gpt-3.5-turbo"]
    max_tokens: 2500
    temperature: 0.2
    
  privacy_focused:
    preferred_models: ["llama3.1:70b", "llama3.1:8b"]
    max_tokens: 3000
    temperature: 0.15
